{"meta":{"title":"梨串桃的生活日记","subtitle":"","description":"感知幸福是一种能力","author":"ChuantaoLi","url":"http://example.com","root":"/"},"pages":[{"title":"我的简介","date":"2024-04-10T15:41:56.000Z","updated":"2026-01-04T08:45:07.597Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"","date":"2026-01-04T08:20:20.272Z","updated":"2026-01-04T08:20:20.272Z","comments":true,"path":"speak/index.html","permalink":"http://example.com/speak/index.html","excerpt":"","text":"#bber { margin-top: 20px; } .bb-list-item { background: #fff; border-radius: 12px; padding: 15px; margin-bottom: 20px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); border: 1px solid #eee; } .bb-content { font-size: 15px; color: #333; line-height: 1.6; } .bb-datetime { font-size: 12px; color: #999; margin-top: 8px; } var bbMemos = { memos : 'https://lct-memos.pages.dev/',//修改为自己部署 Memos 的网址，末尾有 / 斜杠 limit : '',//默认每次显示 10 条 creatorId:'1' ,//早期默认为 101 用户，新安装是 1； https://demo.usememos.com/u/101 domId: '#bber',//默认为 bber twiEnv:'',//启开 twikoo 评论，默认 https://metk.edui.fun/ }"},{"title":"生活记录","date":"2026-01-04T17:16:55.266Z","updated":"2026-01-04T17:16:55.266Z","comments":true,"path":"memos/index.html","permalink":"http://example.com/memos/index.html","excerpt":"","text":"#bber { margin-top: 20px; font-family: 'LXGW WenKai Screen', sans-serif; } .bb-list-item { background: rgba(255, 255, 255, 0.85); border-radius: 20px; padding: 25px; margin-bottom: 24px; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08); transition: transform 0.3s ease; } .bb-list-item:hover { transform: translateY(-5px); } .bb-content { font-size: 15px; color: #333; line-height: 1.6; } .bb-datetime { font-size: 12px; color: #999; margin-top: 12px; border-top: 1px dashed #eee; padding-top: 8px; } /* --- 图片展示优化设计 --- */ .bb-content img { cursor: zoom-in; /* 鼠标移动上去显示放大镜图标 */ max-width: 400px; /* 限制图片最大宽度，不至于撑满屏幕 */ max-height: 300px; /* 限制高度，防止长图过长 */ object-fit: cover; /* 保持图片比例缩放 */ border-radius: 12px; margin: 10px 0; display: block; box-shadow: 0 4px 12px rgba(0,0,0,0.1); transition: opacity 0.3s ease; } .bb-content img:hover { opacity: 0.8; } /* 鼠标悬停略微变暗提示可点击 */ /* 适配手机端：图片自动充满宽度 */ @media screen and (max-width: 768px) { .bb-content img { max-width: 100%; max-height: 400px; } } 正在加载备忘录… var bbMemos = { memos: 'https://memos-api.lct-memos.workers.dev/', limit: '10', creatorId: '1', domId: '#bber', } async function loadMemos() { const bber = document.querySelector(bbMemos.domId); // 适配 Memos v1 API 路径（如果加载不出，尝试去掉末尾的 s） const fetchUrl = `${bbMemos.memos}api/v1/memo?creatorId=${bbMemos.creatorId}&limit=${bbMemos.limit}&rowStatus=NORMAL`; try { const res = await fetch(fetchUrl); const memos = await res.json(); let html = \"\"; memos.forEach(memo => { let content = marked.parse(memo.content); let date = new Date(memo.createdTs * 1000).toLocaleString(); html += ` ${content} ${date} `; }); bber.innerHTML = html; // 初始化图片点击放大功能 window.ViewImage && ViewImage.init('.bb-content img'); } catch (err) { bber.innerHTML = \"加载失败，请检查后端设置。\"; } } window.addEventListener('DOMContentLoaded', loadMemos);"},{"title":"博客类型","date":"2024-04-10T15:40:31.000Z","updated":"2026-01-04T08:39:50.086Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2024-04-10T15:41:25.000Z","updated":"2026-01-04T08:43:35.421Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数据挖掘学习笔记：朴素贝叶斯","slug":"数据挖掘学习笔记：朴素贝叶斯","date":"2026-01-04T14:12:08.000Z","updated":"2026-01-04T16:03:19.374Z","comments":true,"path":"2026/01/04/数据挖掘学习笔记：朴素贝叶斯/","permalink":"http://example.com/2026/01/04/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/","excerpt":"本文记录了我对朴素贝叶斯的学习过程，包含算法的推导和代码实现。","text":"本文记录了我对朴素贝叶斯的学习过程，包含算法的推导和代码实现。 机器学习系列（四）：朴素贝叶斯（华强买瓜版） - yyxy的文章 - 知乎 十分钟，让你再也忘不掉贝叶斯分类 - VoidHaruhi的文章 - 知乎 《机器学习》（西瓜书）公式详解 【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导 到底要如何理解条件概率？ - 石溪的回答 - 知乎 如何在 Python 中从零开始实现朴素贝叶斯 数据挖掘学习笔记：朴素贝叶斯 贝叶斯决策论 假设当前有一个 分类任务，即 ，将 定义为将一个真实标记 的样本误分类为 所产生的损失。如果其目标为最小化分类错误率，则损失 可写为： ，，， 此时，对于单个样本 而言，定义其期望损失为如下条件风险的形式： 上式中， 为后验概率。 那么，贝叶斯决策论的任务就是去寻找一个判定准则 ，以最小化全部样本构成的总体风险： 在定义完上述概念之后，我们就可以引入贝叶斯判定准则，即最小化总体风险 。因此，只需在每个样本上选择那个能使条件风险 最小的类别标记： 此时， 被称为贝叶斯最优分类器（Bayes optimal classifier）。 对公式展开得： 对于一个 分类任务而言，所有类别预测的概率总和一定为 1，即： 此时，条件风险可化简为： 于是，最小化分类错误率的贝叶斯最优分类器可写为： 对每个样本 ，选择能使后验概率 最大的类别标记。 生成式模型与判别式模型 如 SVM 这样的机器学习模型，其本质是在特征空间内寻找一个超平面把类别样本划分开，是一个从几何角度思考的模型，并没有涉及概率的计算。所谓判别式模型，就是直接对后验概率进行建模，求出每个类别的概率进行分类。下面要将的朴素贝叶斯则属于生成式模型，其先对联合概率先进行建模，再推导出后验概率，即： 公式是很简单的条件概率一般定义，可以从古典概型进行推导。 假定一个试验有 个等可能的结果，事件 和 分别包含 个和 个结果，这其中有 个结果是公共的，这就是同时发生事件 和事件 ，即 事件所包含的试验结果数。 那么已知在事件 发生的前提条件下，事件 发生的概率为： 对于上式，可以进行展开： 故可以得到条件概率的一般定义： ​回到公式，用贝叶斯定理可以恒等变形为： 其中， 是先验概率， 是样本 相对于类别标记是类条件概率，也叫似然， 是归一化用的证据因子。 在公式中最难建模的是类条件概率，假如样本 有 个特征，每个特征又有多个取值，那么样本相对于类别 的组合数不胜数，甚至在数据集中都没有这种组合。那如何把这个类条件概率计算出来呢？朴素贝叶斯给了这么一种方法。 朴素贝叶斯 朴素贝叶斯假设对于已知类别，各个属性相互独立，即满足属性条件独立性假设。那么后验概率可以写成： 其中， 为属性数目， 表示 在第 个属性上的取值。基于贝叶斯判定准则可得： 综上所述，样本所属的哪个类别的后验概率最大，就选择哪个类别作为模型 的预测结果。那么，现在的问题就是，如何计算类条件概率和先验概率，才能使得模型预测得最准，即条件风险最小。 对于先验概率的计算很简单，利用大数定理，在训练集样本足够多的情况下用频率近似概率： 其中， 表示训练集 中类别标记为的样本集合， 表示集合 的样本总数。 对于类条件概率的计算要分成两种情况讨论：离散属性和连续属性。对于离散属性的类条件概率计算思路和公式一样，因此第 个属性为离散属性的类条件概率为： 其中， 表示 在第个属性上取值为 的样本组成的集合。 然而，如果某个属性值在训练集中没有与某个类别同时出现过，直接使用公式计算，则会让连乘式子结果为 0。为避免出现这种情况，在估计概率值时应该加以平滑，按《西瓜书》的示例，使用拉普拉斯修正： 其中， 表示类别数量， 表示第 个属性出现的取值数量。 对于连续属性，假设满足正态分布，那么其概率密度函数为： 其中， 和 分别表示第 类样本在第 个属性上取值的均值和方差。 接下来，我们使用极大似然估计去估计其均值和方差。公式为一元正态分布的概率密度函数，下面对于多元正态分布的概率密度函数 而言，其等价形式为： 其中， 表示 的维数， 为正定协方差矩阵。 对数极大似然估计的参数求解形式为： 通过极大似然估计的均值和方差如下： 详细的推导过程见南瓜书第 63~64 页。 假如说，将朴素贝叶斯应用于你的数据集上发现效果并不理想，那么可以考虑更换概率分布，这或许可以看作是贝叶斯分类的超参数。当然，也有可能数据集上的各个属性不独立。 实现代码 代码中使用的数据集来自和鲸社区：皮马印第安人糖尿病数据库。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import mathimport pandas as pd# 皮马印第安人糖尿病数据train_data = pd.read_csv(r'train_data.csv').valuestest_data = pd.read_csv(r'test_data.csv').values# 划分连续和离散型变量continuous_columns = [1, 2, 3, 4, 5, 6] # Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunctiondiscrete_columns = [0, 7] # Pregnancies, Age# 将数据按类别分离separated_data = {0: [], 1: []} # 创建一个空字典用来存储两个类别下的样本for row in train_data: class_label = row[-1] # 获取类别列 separated_data[class_label].append(row) # 把相同类别下的样本放到一起# 分别计算两个类别下每个连续变量的均值和标准差、和离散变量的频率class_summaries = {} # 创建一个空字典用来存储两个类别下连续变量和离散变量的统计信息for class_label, rows in separated_data.items(): summaries = {} # 连续型变量的均值和标准差 continuous_summaries = [] for col_idx in continuous_columns: column = [row[col_idx] for row in rows] # 把连续变量的一整列提取出来 mean_val = sum(column) / len(column) # 计算均值 μ_c variance = sum([(x - mean_val) ** 2 for x in column]) / len(column) # 计算方差 σ_c^2 stdev_val = math.sqrt(variance) # 计算标准差 σ_c continuous_summaries.append((mean_val, stdev_val)) # 离散型变量的频率计算 discrete_summaries = [] for col_idx in discrete_columns: column = [row[col_idx] for row in rows] # 把离散变量的一整列提取出来 value_counts = {val: column.count(val) for val in set(column)} # 统计离散变量中每个取值出现的个数，加1是拉普拉斯修正 total_count = len(rows) discrete_summaries.append((value_counts, total_count)) # 存储两个类别下连续型和离散型的特征统计信息 summaries['continuous'] = continuous_summaries summaries['discrete'] = discrete_summaries class_summaries[class_label] = summaries# 拉普拉斯修正后的先验概率class_prior = {}total_samples = len(train_data) # 样本长度 |D|num_classes = len(class_summaries) # 类别数 Nfor class_label in separated_data: # 取出两个类别的样本 class_count = len(separated_data[class_label]) # 两个类别各自的样本数量 |D_c| class_prior[class_label] = (class_count + 1) / (total_samples + num_classes)# 分类predictions = []for row in test_data: # 对测试集的每一个样本进行分类 probabilities = {} for class_label, summaries in class_summaries.items(): # 两个类别下连续变量和离散变量的统计信息 # 将第一个类条件概率初始化为先验概率 probabilities[class_label] = class_prior[class_label] # 连续变量的类条件概率 for i, (mean_val, stdev_val) in enumerate(summaries['continuous']): x = row[continuous_columns[i]] # 测试集每个样本中每个连续变量的取值 exponent = math.exp(-((x - mean_val) ** 2 / (2 * stdev_val ** 2))) continous_probability = (1 / (math.sqrt(2 * math.pi) * stdev_val)) * exponent probabilities[class_label] *= continous_probability # 离散变量的类条件概率 for i, (value_counts, total_count) in enumerate(summaries['discrete']): x = row[discrete_columns[i]] num_values = len(value_counts) # 变量取值的数量 N_i discrete_probability = (value_counts.get(x, 0) + 1) / (total_count + num_values) # 拉普拉斯修正 probabilities[class_label] *= discrete_probability # 计算归一化因子 P(x) total_probability = sum(probabilities.values()) for class_label in probabilities: probabilities[class_label] /= total_probability best_class = max(probabilities, key=probabilities.get) # 看看哪个类别的后验概率大 predictions.append(best_class)# 计算准确率correct_predictions = sum(1 for i in range(len(test_data)) if test_data[i][-1] == predictions[i])accuracy = (correct_predictions / len(test_data)) * 100.0print(f'code_accuracy: {accuracy:.4f}%') 半朴素贝叶斯 朴素贝叶斯是基于各属性都相互独立的假设进行的，但现实生活中并没有如此理想。半朴素贝叶斯则适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。 回顾公式，朴素贝叶斯的类条件概率考虑的是只与一个属性 有关，而半朴素贝叶斯不仅考虑属性 ，还考虑其他的一个属性，即独依赖估计（ODE）： 其中， 表示属性 所依赖的属性，称为 的父属性。 对于如何确定父属性，最直接的做法是假设所有属性都依赖于同一个属性，即超父独依赖估计（SPODE）： 其中， 为超父属性。","categories":[{"name":"算法学习","slug":"算法学习","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"《Optimal Discriminant Support Vector Machine》阅读笔记","slug":"Optimal-Discriminant-Support-Vector-Machine阅读笔记","date":"2026-01-04T11:58:29.000Z","updated":"2026-01-04T16:03:05.772Z","comments":true,"path":"2026/01/04/Optimal-Discriminant-Support-Vector-Machine阅读笔记/","permalink":"http://example.com/2026/01/04/Optimal-Discriminant-Support-Vector-Machine%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","excerpt":"本文是对《Optimal Discriminant Support Vector Machine》这篇文章的阅读笔记，推导了二分类情景下文章所提模型目标函数的设计和求解过程。","text":"本文是对《Optimal Discriminant Support Vector Machine》这篇文章的阅读笔记，推导了二分类情景下文章所提模型目标函数的设计和求解过程。 Zhang, J., Lai, Z., Kong, H., &amp; Yang, J. (2025). Learning the optimal discriminant SVM with feature extraction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(4), 2897–2911. https://doi.org/10.1109/TPAMI.2025.3529711. 模型总览 ODSVM 将子空间学习和 SVM 分类放在一个统一的数学框架内共同进化。 投影矩阵：数据的特征提取器 是一个大小为 的矩阵（ 是原始高维特征维度， 是降维后的维度）。它的作用是将原始样本 投影到低维子空间： 的目标在于丢弃大部分冗余信息的同时，精准地保留最有利于分类的信息。在 ODSVM 中， 的构建受到 SVM 边距最大化的约束。这意味着 提取出的特征 必须让 SVM 能够更容易地找到一个宽阔的分类间隔（Margin）。 重构矩阵：数据的结构保持器 同样是一个 的矩阵，用于衡量投影后的低维特征 是否还保留了原始数据 的核心结构。数学上表现为最小化重构误差：。 如果只追求分类准确率（只优化 和 SVM 参数），模型可能会陷入过拟合，或者提取出极其扭曲的特征。 的存在就像是一条绳子，拉住 ，要求提取的特征不仅要能分得开类别，还得能代表原本的数据。 迭代优化 在 ODSVM 的目标函数中，SVM 的参数（）、投影矩阵 和重构矩阵 是紧密耦合在一起的。由于无法直接求出所有变量的最优解，因此论文采用交替迭代优化（Alternating Optimization）的策略。 第一步：固定 和 ，优化 SVM，找最强判别平面 此时， 已经把数据变成了低维特征 。模型退化成一个标准的 SVM 问题。当前的子空间里，找到能把不同类别分得最开的超平面 。 第二步：固定 SVM 参数和 ，优化 需要同时满足两个要求：SVM 要求投影 得让它的分类间隔更大， 要求投影 要能配合重构矩阵更好地还原回 。在这种约束下， 会被微调，使得投影后的数据点在保持结构的同时，向着更有利于分类的方向偏移。 第三步：固定 和 SVM，优化 由于 已经变了，原来的恢复方案 可能不再准确。这一步更新 ，使得它能根据新的低维特征 更好地重建原始数据。 图正则化重构 ODSVM 并不是直接最小化低维点的距离，而是通过最小化邻居编码误差（Neighbor-encoding）来间接抑制低维空间中邻居间的距离。 构建有监督图 在进入迭代优化前，模型先为所有训练样本构建一个图 ，利用了类别信息和欧氏距离： 同类近邻： 如果样本 和 属于同一类，且 是 的 个最近邻之一，那么就在它们之间连一条边，并赋予一个权重 。 异类或远邻： 如果类别不同或者距离太远，权重 。 邻域编码 在构建子空间（学习 和 ）时，目标函数不再仅仅是最小化 ，而是加入了基于图的约束。如果 很大，模型会强制要求： 在低维空间 中靠得更近： 和 的距离必须尽可能小。 共享重构特征： 它们的重构过程应该具有一致性。 对于投影矩阵 要求局部紧致性（Local Compactness）。 在寻找投影方向时，会被要求最小化同类邻居在子空间中的方差。如果 选的方向让原本在一起的同类样本分开了，那么图正则化项就会产生很高的“惩罚值”。因此， 提取出的特征能够保持数据的局部流形结构，即在原始高维空间里邻近的同类点，在低维子空间依然是邻居。 对于重构矩阵 要求结构一致性。 不仅仅是一个数学上的逆变换，它被赋予了“平滑”数据的任务。通过考虑邻域， 学习到的是该类别数据在子空间中的分布规律，而不仅仅是单个点的坐标映射。因此，如果某个样本 含有噪声，但它的邻居们是干净的，邻域编码会利用邻居的结构信息来辅助 的重构，从而“纠正”噪声样本在子空间中的偏离。 优化过程 以二分类为例，定义目标函数： 分类项图正则化重构项正则化项 约束条件为：。 其中， 是投影矩阵， 是重构字典， 是低维空间中的分类权重， 是刻画局部结构的图权重矩阵。 用于平衡“最大化分类间隔”和“最小化分类错误”之间的权重， 越大，对分类错误的容忍度越低。 用于控制“特征提取/结构保持”这一项在总目标函数中的重要性， 越大，模型越强调整体数据的结构保持和类内紧凑性。 用于控制投影矩阵的复杂度。 是取正值的意思，在 SVM 中用来定义 Hinge Loss，若分对了且离边界够远，就不产生 Loss。分类错误或分类正确但离边界太近，损失就为正数，离正确边界越远，惩罚越大，迫使模型去修正参数。 是标准的 SVM 损失函数，但输入数据变成了 ，这意味着模型在寻找 和 使得数据在投影后能被最大间隔分开（最大化类间间隔）。 这一项利用邻居 的投影 ，通过 映射回来去重构 ，同类样本在低维空间应该靠得很近，并且能保留原始数据的信息。这起到了“最小化类内散度”的作用。 是正则化项，防止投影矩阵 的值过大。 第一阶段：固定投影矩阵和重构字典，优化 SVM 当 和 固定时，目标函数中与 相关的重构项和正则项都变成了常数。此时问题退化为： 这就是一个标准的线性 SVM 训练问题。目标是在投影后的低维子空间内，找到一个超平面系数 ，使得分类间隔（Margin）最大化。 第二阶段：固定 SVM 参数和重构字典，优化投影矩阵 因为 同时存在于分类项和重构项中。为了求解，作者引入松弛变量 并利用对偶理论。 首先进行目标函数重构，利用矩阵迹（Trace）的性质，重构项可以转化为： 其中 和 ，这两个矩阵被称为散射矩阵（Scatter Matrices）。 通常与数据的全局能量或总方差有关，而 捕获了基于图结构的局部相关性。 是度矩阵（Degree Matrix），是一个对角矩阵，其对角线上的元素 是 矩阵第 行（或列）元素的总和： 在矩阵运算中，迹 是矩阵对角线元素之和。 向量或矩阵的二范数平方可以写成迹的形式：。对于矩阵 ：。 矩阵的迹还有循环移位的性质：。如果一个运算的结果是标量，那么它的迹等于它本身：。 推导过程 首先利用向量范数公式 ，将目标函数展开为三项： 第一项第二项第三项 第一项是常数项，由于 和 在优化阶段都是已知的，这一项不包含变量 和 ，因此记为 const： 在第二项中，由于 是一个标量，它等于自身的迹： 利用迹的循环性质 ，将 移到前面： 由于迹运算对求和是线性的，提取 和 ： 由于 正好是矩阵乘法 的展开形式，定义 ，则该项为： 第三项首先利用正交约束 简化内部项： 的作用是将低维特征 “还原”回原始空间。如果没有 这个约束，模型可以通过无限缩小 并无限放大 来保持乘积不变。这会导致计算极其不稳定。正交基意味着模型是在一个标准直角坐标系下进行重构，这保证了重构过程只是在寻找原始数据在子空间上的投影点，而不会对数据产生非线性的拉伸或剪切。 对于投影矩阵而言， 的角色是寻找最有利于分类的特征，虽然 没有正交约束，但它有 正则化项。这里的正则化是 Frobenius 范数，把矩阵里所有的元素先平方、再求和、最后开根号。Frobenius 范数限制了投影矩阵 中元素的大小。如果没有这一项， 的数值可能会为了迎合某些噪声数据而变得极大，导致模型泛化能力变差： 回到公式的推导，代回原式并考虑对 的求和： 利用度矩阵定义 ： 转化为迹的形式并利用循环性质： 由于 正好是矩阵乘法 的对角加权形式，定义 ，则该项为： 将三项合并，得到了论文中的简洁形式： 在固定 和 后，ODSVM 的目标函数 与变量 和松弛变量 相关的部分可以写成： 重构项的正则项误差项 约束条件来自于 SVM 的基本要求，即样本必须落在间隔之外或通过松弛变量补偿：，松弛变量非负：。 拉格朗日乘数法要求约束条件写成 的形式：，。 拉格朗日函数的基本构造公式为：目标函数乘子约束。 引入两组拉格朗日乘子： 对应分类约束， 对应非负约束： 进一步把上式进行化简： 是对分类错误的惩罚总和，其中 ，所以 。这里省略了 项，是因为后续的求偏导过程有： 由于 ，这个条件隐含了 。因此，针对 优化拉格朗日函数时通常可以省略 项，通过在后续求解 的对偶问题时，给 加上上限 （即 ）来等价地处理 的约束。 接着求导得出 的显式表达，令 ，可以解出 关于 的函数。 首先，从完整的拉格朗日函数中，剔除掉所有不包含 的项，因为求导后会变成0，得到关于 的子函数 ： 项项项项 在推导过程需要用到以下三个矩阵求导公式： 公式 A： 。由于 是对称矩阵，结果为 。 公式 B： 。 公式 C： 。 将 分成四部分进行计算： 求导项 1： 根据公式 A，由于 是对称的： 项 求导项 2： 根据公式 B，其中 ： 项 求导项 3： 这等价于公式 A 中 的情况： 项 求导项 4： 这一步需要先进行矩阵化处理。 观察单个项：。这里 是分类权重， 是样本。 利用公式 C，其中 ， 。其导数为 。 对所有 求和： 项 引入矩阵 和向量 ，上面的求和项 正好等于矩阵乘积 。所以有： 项 将上述所有结果合并，得到总梯度： 化简后得到： 这里包含三部分： 要求投影矩阵配合重构字典 和图结构 ，尽量保留数据的原始几何信息 要求投影矩阵参考 SVM 的分类结果，把不同类别的样本拉开，其通过拉格朗日乘子 和权重 传递 则起到了一个“归一化”和“稳定器”的作用，确保 不会因为单方面的力量而数值爆炸 当得到了上述投影矩阵的显示表达后，接下来要求解针对 的二次规划问题（QPP），需要将第二阶段求得的 的显式表达代回拉格朗日函数中，这个过程被称为对偶化（Dualization）。 首先对拉格朗日函数进行简化： 上式合并了重构项和 的正则项，使用 ，以及将分类约束项 写成了矩阵形式：。 利用 Karush-Kuhn-Tucker（KKT） 条件简化 和约束，先针对松弛变量 求偏导： 因为乘子 ，这直接给出了 的范围约束： 同时，由于 ，拉格朗日函数中的 项变为 0。 把在上一阶段求得 的最优解化简为： 其中 。 将 代入拉格朗日函数中关于 的部分： 由于 是对称矩阵，因此 也是对称矩阵，有 ，简化得： 将 代入： 展开后得到四项： 第一项： 。这不包含 ，在优化时是常数。 第二项： 。 第三项： 。 第四项： 。 接下来对上述四项分别求迹，因为第二项和第三项互为转置，因此： 对于第四项利用 ，得到： 回到一开始的拉格朗日函数： 刚刚的化简和求迹都是针对 进行的， 被放在了一边，现在计算完成后需要将结果和 合并： 最大化这个对偶函数 ，即得到了论文中的 QPP 问题： 这里的二次项 反映了样本在经过 缩放处理后的相关性， 的存在说明分类器的权重直接影响了子空间的拉伸。一次项 表示标准 SVM 要求样本离开超平面，而减去的 部分则是来自重构误差的修正。它告诉模型，如果某个样本为了重构而必须待在某个位置，那么 SVM 对它的分类约束可以稍微“妥协”一点。 第三阶段：固定投影矩阵和 SVM，优化重构字典 当 和 固定时，目标函数只剩下与重构相关的部分。 这里把最开始目标函数的简洁形式搬过来： 在这一阶段要优化的重构形式为： 这是一个经典的正交普鲁克分析（Orthogonal Procrustes Problem），它有闭式解（Closed-form solution）： 首先计算矩阵乘积 ，对该结果进行奇异值分解（SVD）：，得到的最优的重构矩阵为：。","categories":[{"name":"文献阅读","slug":"文献阅读","permalink":"http://example.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://example.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"},{"name":"子空间学习","slug":"子空间学习","permalink":"http://example.com/tags/%E5%AD%90%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0/"}]}],"categories":[{"name":"算法学习","slug":"算法学习","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"},{"name":"文献阅读","slug":"文献阅读","permalink":"http://example.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://example.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"},{"name":"子空间学习","slug":"子空间学习","permalink":"http://example.com/tags/%E5%AD%90%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0/"}]}