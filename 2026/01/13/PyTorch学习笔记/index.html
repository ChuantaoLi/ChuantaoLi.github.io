<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 8.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文记录了我对PyTorch的学习过程，参考B站小土堆课程。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch学习笔记">
<meta property="og:url" content="http://example.com/2026/01/13/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="梨串桃的生活日记">
<meta property="og:description" content="本文记录了我对PyTorch的学习过程，参考B站小土堆课程。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2026-01-13T13:51:19.000Z">
<meta property="article:modified_time" content="2026-01-13T13:53:09.371Z">
<meta property="article:author" content="ChuantaoLi">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2026/01/13/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PyTorch学习笔记 | 梨串桃的生活日记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="梨串桃的生活日记" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">梨串桃的生活日记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-说说">

    <a href="/memos/" rel="section"><i class="far fa-comment-dots fa-fw"></i>说说</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2026/01/13/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/%E7%8C%AB%E5%92%AA%E5%A4%B4%E5%83%8F.jpg">
      <meta itemprop="name" content="ChuantaoLi">
      <meta itemprop="description" content="感知幸福是一种能力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="梨串桃的生活日记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2026-01-13 21:51:19 / 修改时间：21:53:09" itemprop="dateCreated datePublished" datetime="2026-01-13T21:51:19+08:00">2026-01-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">算法学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <div class="note success">
            <p>本文记录了我对PyTorch的学习过程，参考B站小土堆课程。</p>
          </div>
<span id="more"></span>
<h1 id="pytorch学习笔记">PyTorch学习笔记</h1>
<h2 id="数据读取">数据读取</h2>
<h3 id="使用dataset进行数据读取">使用Dataset进行数据读取</h3>
<p>在PyTorch中，<code>Dataset</code>是一个抽象基类，其规定了所有数据集必须具备两个功能：</p>
<ul>
<li><p><code>__len__</code>：告诉模型这里一共有多少张图</p></li>
<li><p><code>__getitem__</code>：告诉模型一个编号idx，把对应的图片和标签给你</p></li>
</ul>
<p><code>__getitem__</code>
的按需读取并没有在<code>init</code>中把所有图片都加载进内存，只有当代码运行到<code>dataset[idx]</code>时，<code>Image.open</code>才会去磁盘读取这一张图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  <span class="comment"># 导入Dataset基类</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">定义一个名为MyData的类，继承PyTorch提供的Dataset类</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># 初始化方法：用于获取数据的信息</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dirt, label_dirt</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param root_dirt: 数据集的根目录路径</span></span><br><span class="line"><span class="string">        :param label_dirt: 标签的文件夹名称</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="variable language_">self</span>.root_dirt = root_dirt</span><br><span class="line">        <span class="variable language_">self</span>.label_dirt = label_dirt</span><br><span class="line">        <span class="comment"># 使用os.path.join拼接路径</span></span><br><span class="line">        <span class="variable language_">self</span>.path = os.path.join(<span class="variable language_">self</span>.root_dirt, <span class="variable language_">self</span>.label_dirt)</span><br><span class="line">        <span class="comment"># 获取该目录下所有文件的文件名，以列表的形式存储</span></span><br><span class="line">        <span class="variable language_">self</span>.img_path = os.listdir(<span class="variable language_">self</span>.path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取单个数据样本的方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param idx: 数据的编号，如第0张图，第1张图</span></span><br><span class="line"><span class="string">        :return: 返回处理后的图像和对应的标签</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 1. 根据索引idx从文件夹列表中获取对应的文件名</span></span><br><span class="line">        img_name = <span class="variable language_">self</span>.img_path[idx]</span><br><span class="line">        <span class="comment"># 2. 拼接出完整路径</span></span><br><span class="line">        img_item_path = os.path.join(<span class="variable language_">self</span>.root_dirt, <span class="variable language_">self</span>.label_dirt, img_name)</span><br><span class="line">        <span class="comment"># 3. 使用PIL库的open打开图片文件</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        <span class="comment"># 4. 获取标签，使用文件夹名称作为标签名称</span></span><br><span class="line">        label = <span class="variable language_">self</span>.label_dirt</span><br><span class="line">        <span class="comment"># 5. 返回一个元组：（图片，标签）</span></span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取数据集样本的长度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">代码测试</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 1. 设置数据集所在的绝对路径</span></span><br><span class="line">root_dirt = <span class="string">r"D:\RecommendationSystem\PyTorch学习\hymenoptera_data\train"</span></span><br><span class="line">ants_label_dir = <span class="string">"ants"</span>  <span class="comment"># 蚂蚁文件夹</span></span><br><span class="line">bees_label_dir = <span class="string">"bees"</span>  <span class="comment"># 蜜蜂文件夹</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建蚂蚁数据集的实例对象</span></span><br><span class="line">ants_dataset = MyDataSet(root_dirt, ants_label_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 访问数据集，通过索引触发__getitem__方法</span></span><br><span class="line">img, label = ants_dataset[<span class="number">0</span>]</span><br><span class="line">img.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="使用dataloader进行批量读取">使用DataLoader进行批量读取</h3>
<p><code>DataLoader</code>用于批量读取，在神经网络中常常输入一个batch进行训练。常见的参数有<code>Batch Size</code>，表示批次大小；<code>Shuffle</code>，表示输入前对样本顺序进行洗牌。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir, transforms=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.root_dir = root_dir</span><br><span class="line">        <span class="variable language_">self</span>.label_dir = label_dir</span><br><span class="line">        <span class="variable language_">self</span>.path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.label_dir)</span><br><span class="line">        <span class="variable language_">self</span>.img_path = os.listdir(<span class="variable language_">self</span>.path)  <span class="comment"># 用os的listdir获取数据集的图片列表</span></span><br><span class="line">        <span class="variable language_">self</span>.transform = transforms  <span class="comment"># 输入图像转换操作</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name = <span class="variable language_">self</span>.img_path[idx]  <span class="comment"># 根据idx获取单张图片</span></span><br><span class="line">        img_item_path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.label_dir, img_name)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path).convert(<span class="string">"RGB"</span>)  <span class="comment"># 强制转换成三通道</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            img = <span class="variable language_">self</span>.transform(img)</span><br><span class="line"></span><br><span class="line">        label = <span class="variable language_">self</span>.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个变换，因为DataLoader的输入是张量</span></span><br><span class="line">img_transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),  <span class="comment"># 进行缩放</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转，数据增强,</span></span><br><span class="line">        transforms.ToTensor(),  <span class="comment"># 转换为张量</span></span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], std=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),  <span class="comment"># 标准化</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建蚂蚁数据集实例</span></span><br><span class="line">root_dir = <span class="string">r"D:\RecommendationSystem\PyTorch学习\hymenoptera_data\train"</span>  <span class="comment"># 根目录，存储数据级的目录</span></span><br><span class="line">ants_label_dir = <span class="string">"ants"</span>  <span class="comment"># 蚂蚁类别的数据集目录</span></span><br><span class="line">bees_label_dir = <span class="string">"bees"</span>  <span class="comment"># 蜜蜂类别的数据集目录</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir, transforms=img_transforms)</span><br><span class="line"></span><br><span class="line">MyDataLoader = DataLoader(</span><br><span class="line">    dataset=ants_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> MyDataLoader:</span><br><span class="line">    imgs, labels = data</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"当前批次的图片形状：<span class="subst">{imgs.shape}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="使用transforms进行数据转换">使用Transforms进行数据转换</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载图片</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">r"C:\Users\13680\Pictures\猫咪头像.jpg"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 从图片到张量 (ToTensor)</span></span><br><span class="line"><span class="comment"># 目的：将 PIL 图片转为模型能处理的浮点张量，并将像素范围从 [0, 255] 归一化到 [0, 1]</span></span><br><span class="line"><span class="comment"># 还会自动把维度顺序从 [H, W, C] 调整为 [C, H, W]</span></span><br><span class="line">to_tensor = transforms.ToTensor()</span><br><span class="line">img_tensor = to_tensor(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"张量形状 [通道, 高, 宽]: <span class="subst">{img_tensor.shape}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 尺寸缩放 (Resize)</span></span><br><span class="line"><span class="comment"># 目的：统一输入尺寸。模型网络层的参数是固定的，必须接受相同大小的图片</span></span><br><span class="line">resize = transforms.Resize((<span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line">img_resized = resize(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"缩放后形状: <span class="subst">{to_tensor(img_resized).shape}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 中心裁剪 (CenterCrop)</span></span><br><span class="line"><span class="comment"># 目的：移除背景干扰，只关注图片最中心的核心特征，比如猫脸</span></span><br><span class="line">centercrop = transforms.CenterCrop(<span class="number">300</span>)</span><br><span class="line">img_croped = centercrop(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 随机裁剪 (RandomCrop)</span></span><br><span class="line">randomcrop = transforms.RandomCrop(size=(<span class="number">100</span>, <span class="number">200</span>))</span><br><span class="line">img_randomcroped = randomcrop(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 随机水平翻转 (RandomHorizontalFlip)</span></span><br><span class="line"><span class="comment"># 目的：模拟物体从不同方向出现的情况。p=0.5 表示一半的概率翻转，一半不翻</span></span><br><span class="line">random_horizontal_flip = transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>)</span><br><span class="line">flipped_image = random_horizontal_flip(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图展示对比</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">"Original Image"</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)  <span class="comment"># 关闭坐标轴</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">"Flipped Image"</span>)</span><br><span class="line">plt.imshow(flipped_image)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 填充 (Pad)</span></span><br><span class="line"><span class="comment"># 目的：给图片加边框。在某些检测任务中，为了保持比例会先填充再缩放</span></span><br><span class="line">pad = transforms.Pad(padding=<span class="number">10</span>, fill=<span class="number">0</span>)  <span class="comment"># 上下左右各填充 10 像素，黑色填充</span></span><br><span class="line">img_padded = pad(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"填充后形状: <span class="subst">{to_tensor(img_padded).shape}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 标准化 (Normalize)</span></span><br><span class="line"><span class="comment"># 公式：output = (input - mean) / std</span></span><br><span class="line"><span class="comment"># 目的：使数据分布在 0 附近。均值 0.5，标准差 0.1 是为了让大多数像素落在 [-5, 5] 之间</span></span><br><span class="line">mean = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">std = [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]</span><br><span class="line">normalize = transforms.Normalize(mean=mean, std=std)</span><br><span class="line">img_normalized = normalize(to_tensor(img))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"标准化后的像素均值: <span class="subst">{img_normalized.mean()}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. 操作组合 (Compose)</span></span><br><span class="line"><span class="comment"># 目的：将上述所有步骤串联成一个流水线，以后只需调用 img_transforms(img)</span></span><br><span class="line">img_transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),  <span class="comment"># 1. 缩放</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),  <span class="comment"># 2. 增强</span></span><br><span class="line">        transforms.ToTensor(),  <span class="comment"># 3. 转张量</span></span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], std=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]),  <span class="comment"># 4. 标准化</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">img_transformed = img_transforms(img)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="神经网络">神经网络</h2>
<h3 id="module的使用">Module的使用</h3>
<p>输出通道 （Out
Channels）其实就是卷积核（Filter）的数量。每一个卷积核负责寻找一种特征。设置64个输出通道，就代表想让模型在这一层提取64种不同的高级特征。</p>
<p>当<code>kernel_size=3</code>，<code>padding=1</code>，<code>stride=1</code>时，卷积后的图片长宽不变，公式如下：
<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.602ex;" xmlns="http://www.w3.org/2000/svg" width="50.155ex" height="4.726ex" role="img" focusable="false" viewBox="0 -1381 22168.3 2089"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mi" transform="translate(763,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1335,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1696,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(2199,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2771,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(3409.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4465.6,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(504,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1104,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(1607,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2179,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2762.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(3762.4,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(4484.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(5484.9,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(6235.9,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(6764.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(7284.9,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(7804.9,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8149.9,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8749.9,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(9449.1,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(10449.3,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(11338.3,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(11804.3,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(12255.3,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(12855.3,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(13321.3,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(13619.3,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(14264.3,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(14609.3,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(15074.3,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(15762.6,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(16762.8,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mrow" transform="translate(7457.4,-686)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1006,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1457,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1802,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(2322,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g></g><rect width="17462.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span>
这让我们在设计网络时，只需要担心“池化”带来的减半，而不需要担心卷积层让图片变小。</p>
<p>如果没有激活函数<code>F.relu</code>，无论你堆叠多少层卷积，结果依然只是一个复杂的线性公式。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mymodel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 1. 卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        输入通道数，输出通道数，填充大小</span></span><br><span class="line"><span class="string">        后一层的输入通道数要等于前一层的输出通道数</span></span><br><span class="line"><span class="string">        padding = (kernel_size - 1) / 2</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 池化层</span></span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        kernel_size是窗口大小，stride是窗口每次滑动的距离，二者相等时，池化后的图像大小减半</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">64</span> * <span class="number">64</span>, <span class="number">256</span>)  <span class="comment"># 假设输入图像的形状：(3, 512, 512)</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">256</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 卷积 -&gt; 激活 -&gt; 池化</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool(</span><br><span class="line">            F.relu(<span class="variable language_">self</span>.conv1(x))</span><br><span class="line">        )  <span class="comment"># 卷积后：(16, 512, 512)；池化后：(16, 256, 256)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool(</span><br><span class="line">            F.relu(<span class="variable language_">self</span>.conv2(x))</span><br><span class="line">        )  <span class="comment"># 卷积后：(32, 256, 256)；池化后：(32, 128, 128)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool(</span><br><span class="line">            F.relu(<span class="variable language_">self</span>.conv3(x))</span><br><span class="line">        )  <span class="comment"># 卷积后：(64, 128, 128)；池化后：(64, 64, 64)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 展平，进入全连接层前，需要把多维张量拉成一维向量</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Mymodel(num_classes=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="池化的使用">池化的使用</h3>
<p>最大池化只看最强的信号，它能很好地提取边缘、纹理等剧烈变化的特征。平均池化
考虑所有像素，常用于网络的最后一层，用来整合全局信息。</p>
<p>一个非常普遍的工程习惯是<code>kernel_size == stride</code>。如果<code>kernel_size=2</code>，<code>stride=2</code>，图片尺寸直接除以2。如果<code>stride</code>小于<code>kernel_size</code>，池化窗口会产生重叠，这在如AlexNet等某些老模型中被认为能抑制过拟合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟一个 4x4 的输入特征图 (Batch=1, Channel=1, H=4, W=4)</span></span><br><span class="line">input_data = torch.tensor(</span><br><span class="line">    [[[[<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">30</span>, <span class="number">40</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">100</span>, <span class="number">80</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">90</span>, <span class="number">70</span>]]]],</span><br><span class="line">    dtype=torch.float32,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 最大池化 (Max Pooling)</span></span><br><span class="line"><span class="comment"># 目的：保留窗口内最显著的特征（数值最大的那个）</span></span><br><span class="line"><span class="comment"># 窗口 2x2，步长 2：意味着图片长宽都会减半</span></span><br><span class="line">max_pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 平均池化 (Average Pooling)</span></span><br><span class="line"><span class="comment"># 目的：保留窗口内的整体背景信息（计算平均值）</span></span><br><span class="line">avg_pool = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行池化</span></span><br><span class="line">output_max = max_pool(input_data)</span><br><span class="line">output_avg = avg_pool(input_data)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--- 原始输入 (4x4) ---"</span>)</span><br><span class="line"><span class="built_in">print</span>(input_data[<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n--- 最大池化结果 (2x2) ---"</span>)</span><br><span class="line"><span class="built_in">print</span>(output_max[<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n--- 平均池化结果 (2x2) ---"</span>)</span><br><span class="line"><span class="built_in">print</span>(output_avg[<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">--- 原始输入 (4x4) ---</span></span><br><span class="line"><span class="string">tensor([[ 10.,  20.,   5.,   0.],</span></span><br><span class="line"><span class="string">        [ 30.,  40.,   0.,   5.],</span></span><br><span class="line"><span class="string">        [  1.,   2., 100.,  80.],</span></span><br><span class="line"><span class="string">        [  0.,   1.,  90.,  70.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">--- 最大池化结果 (2x2) ---</span></span><br><span class="line"><span class="string">tensor([[ 40.,   5.],</span></span><br><span class="line"><span class="string">        [  2., 100.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">--- 平均池化结果 (2x2) ---</span></span><br><span class="line"><span class="string">tensor([[25.0000,  2.5000],</span></span><br><span class="line"><span class="string">        [ 1.0000, 85.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="线性层的使用">线性层的使用</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟一个卷积层输出的特征图 (Batch=1, Channel=64, H=4, W=4)</span></span><br><span class="line">input_feature_map = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Flatten (展平层) ---</span></span><br><span class="line"><span class="comment"># 将三维特征 (C, H, W) 转化为一维特征向量。默认从 dim=1 开始展平，保留 dim=0 的 Batch 维度。</span></span><br><span class="line">flatten = nn.Flatten()</span><br><span class="line">output_flatten = flatten(input_feature_map)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Linear (线性层/全连接层) ---</span></span><br><span class="line"><span class="comment"># 每一个输出节点都连接了输入的所有节点，用于学习全局组合逻辑。</span></span><br><span class="line">fc = nn.Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">256</span>)</span><br><span class="line">output_fc = fc(output_flatten)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. Dropout (随机失活层) ---</span></span><br><span class="line"><span class="comment"># 强制让网络在训练时不要依赖某些特定的神经元。</span></span><br><span class="line"><span class="comment"># p=0.5 表示每次训练时，随机把一半的神经元“关掉”（置为0）。</span></span><br><span class="line">dropout = nn.Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">output_dropout = dropout(output_fc)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"输入特征图形状: <span class="subst">{input_feature_map.shape}</span>"</span>)  <span class="comment"># [1, 64, 4, 4]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"展平后形状:    <span class="subst">{output_flatten.shape}</span>"</span>)  <span class="comment"># [1, 1024]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"线性层输出形状: <span class="subst">{output_fc.shape}</span>"</span>)  <span class="comment"># [1, 256]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="非线性激活的使用">非线性激活的使用</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟一个包含正数、负数和零的输入张量</span></span><br><span class="line">input_data = torch.tensor([-<span class="number">2.0</span>, -<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. ReLU 激活函数：简单高效</span></span><br><span class="line"><span class="comment"># 逻辑：max(0, x)</span></span><br><span class="line">relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Sigmoid 激活函数：压缩映射</span></span><br><span class="line">sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行激活操作</span></span><br><span class="line">output_relu = relu(input_data)</span><br><span class="line">output_sigmoid = sigmoid(input_data)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"输入数据: <span class="subst">{input_data}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"ReLU 输出: <span class="subst">{output_relu}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"Sigmoid 输出: <span class="subst">{output_sigmoid}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="模型训练">模型训练</h2>
<h3 id="反向传播手动实现">反向传播手动实现</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 初始化</span></span><br><span class="line">x = torch.tensor([<span class="number">0.5</span>])</span><br><span class="line">y = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">w1 = torch.tensor([<span class="number">0.2</span>])</span><br><span class="line">w2 = torch.tensor([<span class="number">0.5</span>])</span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="comment"># --- A. 前向传播 ---</span></span><br><span class="line">    z1 = w1 * x</span><br><span class="line">    a1 = torch.sigmoid(z1)</span><br><span class="line">    z2 = w2 * a1</span><br><span class="line">    a2 = torch.sigmoid(z2)</span><br><span class="line">    C = <span class="number">0.5</span> * (a2 - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># --- B. 反向传播 ---</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># --- 第一步：计算 w2 的梯度 (输出层) ---</span></span><br><span class="line">    <span class="comment"># 公式：dC/dw2 = (dC/da2) * (da2/dz2) * (dz2/dw2)</span></span><br><span class="line">    dC_da2 = a2 - y  <span class="comment"># Loss 对预测值的导数</span></span><br><span class="line">    da2_dz2 = a2 * (<span class="number">1</span> - a2)  <span class="comment"># Sigmoid 导数</span></span><br><span class="line">    dz2_dw2 = a1  <span class="comment"># 因为 z2 = w2 * a1，对 w2 求导剩下 a1</span></span><br><span class="line"></span><br><span class="line">    grad_w2 = dC_da2 * da2_dz2 * dz2_dw2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># --- 第二步：计算 w1 的梯度 (隐含层) ---</span></span><br><span class="line">    <span class="comment"># 公式：dC/dw1 = [(dC/da2) * (da2/dz2) * (dz2/da1)] * (dz1/dw1) * (da1/dz1)</span></span><br><span class="line">    <span class="comment"># 方括号的内容表示半成品a1的改变对总损失C的影响</span></span><br><span class="line">    dz2_da1 = w2</span><br><span class="line">    dz1_dw1 = x</span><br><span class="line">    da1_dz1 = a1 * (<span class="number">1</span> - a1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 链式相乘：(dC/da2 * da2/dz2) 即是上一层的误差传递，再乘以 (dz2/da1 * da1/dz1 * dz1/dw1)</span></span><br><span class="line">    grad_w1 = dC_da2 * da2_dz2 * dz2_da1 * dz1_dw1 * da1_dz1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># --- C. 权重更新 ---</span></span><br><span class="line">    w1 = w1 - learning_rate * grad_w1</span><br><span class="line">    w2 = w2 - learning_rate * grad_w2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印查看每轮的变化</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch + <span class="number">1</span>}</span>: Loss = <span class="subst">{C.item():<span class="number">.4</span>f}</span>, w1 = <span class="subst">{w1.item():<span class="number">.4</span>f}</span>, w2 = <span class="subst">{w2.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="完整训练示例">完整训练示例</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, ConcatDataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 数据集定义 ---</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.root_dir = root_dir</span><br><span class="line">        <span class="variable language_">self</span>.label_dir = label_dir</span><br><span class="line">        <span class="variable language_">self</span>.path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.label_dir)</span><br><span class="line">        <span class="variable language_">self</span>.img_path = os.listdir(<span class="variable language_">self</span>.path)</span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line">        <span class="comment"># ants -&gt; 0, bees -&gt; 1</span></span><br><span class="line">        <span class="variable language_">self</span>.label = <span class="number">0</span> <span class="keyword">if</span> label_dir == <span class="string">"ants"</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name = <span class="variable language_">self</span>.img_path[idx]</span><br><span class="line">        img_item_path = os.path.join(<span class="variable language_">self</span>.path, img_name)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path).convert(<span class="string">"RGB"</span>)  <span class="comment"># 保证三通道</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            img = <span class="variable language_">self</span>.transform(img)</span><br><span class="line">        <span class="keyword">return</span> img, <span class="variable language_">self</span>.label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 配置变换 ---</span></span><br><span class="line">data_transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize((<span class="number">64</span>, <span class="number">64</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. 准备加载器 ---</span></span><br><span class="line">train_root = <span class="string">r"D:\RecommendationSystem\PyTorch学习\hymenoptera_data\train"</span></span><br><span class="line">val_root = <span class="string">r"D:\RecommendationSystem\PyTorch学习\hymenoptera_data\val"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">train_ants = MyData(train_root, <span class="string">"ants"</span>, transform=data_transforms)</span><br><span class="line">train_bees = MyData(train_root, <span class="string">"bees"</span>, transform=data_transforms)</span><br><span class="line">train_loader = DataLoader(</span><br><span class="line">    ConcatDataset([train_ants, train_bees]), batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证集</span></span><br><span class="line">val_ants = MyData(val_root, <span class="string">"ants"</span>, transform=data_transforms)</span><br><span class="line">val_bees = MyData(val_root, <span class="string">"bees"</span>, transform=data_transforms)</span><br><span class="line">val_loader = DataLoader(</span><br><span class="line">    ConcatDataset([val_ants, val_bees]), batch_size=<span class="number">8</span>, shuffle=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. 模型定义 ---</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">16</span> * <span class="number">32</span> * <span class="number">32</span>, <span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = SimpleCNN().to(device)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 5. 训练与评估循环 ---</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># 训练阶段</span></span><br><span class="line">    model.train()  <span class="comment"># 切换到训练模式</span></span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        imgs, labels = imgs.to(device), labels.to(device)</span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        loss = loss_fn(outputs, labels)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估阶段</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 切换到评估模式</span></span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不计算梯度，节省内存和计算量</span></span><br><span class="line">        <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> val_loader:</span><br><span class="line">            imgs, labels = imgs.to(device), labels.to(device)</span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            loss = loss_fn(outputs, labels)</span><br><span class="line">            val_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算准确率</span></span><br><span class="line">            predictions = outputs.argmax(<span class="number">1</span>)  <span class="comment"># 获取概率最大的类别索引</span></span><br><span class="line">            total_accuracy += (predictions == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    avg_acc = total_accuracy / <span class="built_in">len</span>(val_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch}</span>: Train Loss: <span class="subst">{train_loss:<span class="number">.3</span>f}</span>, Val Acc: <span class="subst">{avg_acc:<span class="number">.2</span>%}</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 6. 保存模型 ---</span></span><br><span class="line">os.makedirs(<span class="string">"models"</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">torch.save(model.state_dict(), <span class="string">"models/best_model.pth"</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="损失函数">损失函数</h3>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子保证结果可复现</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========================================</span></span><br><span class="line"><span class="comment"># 场景 A：回归问题 (Regression)</span></span><br><span class="line"><span class="comment"># 常用 L1Loss 或 MSELoss</span></span><br><span class="line"><span class="comment"># ==========================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测值与真实值</span></span><br><span class="line">predict = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">target = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">5.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. L1Loss: 计算平均绝对误差 (Mean Absolute Error, MAE)</span></span><br><span class="line"><span class="comment"># 公式: sum(|predict - target|) / n = (0 + 0 + 2) / 3 = 0.6667</span></span><br><span class="line">loss_l1 = nn.L1Loss()</span><br><span class="line">res_l1 = loss_l1(predict, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. MSELoss: 计算均方误差 (Mean Squared Error)</span></span><br><span class="line"><span class="comment"># 公式: sum((predict - target)^2) / n = (0^2 + 0^2 + 2^2) / 3 = 4 / 3 = 1.3333</span></span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">res_mse = loss_mse(predict, target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--- 回归损失 (Regression Losses) ---"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"L1 Loss (MAE): <span class="subst">{res_l1.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"MSE Loss:      <span class="subst">{res_mse.item():<span class="number">.4</span>f}</span>\n"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========================================</span></span><br><span class="line"><span class="comment"># 场景 B：分类问题 (Classification)</span></span><br><span class="line"><span class="comment"># 常用 CrossEntropyLoss</span></span><br><span class="line"><span class="comment"># ==========================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们在做一个三分类任务（如：猫、狗、猪）</span></span><br><span class="line"><span class="comment"># 输入 x: 模型给出的每个类别的得分 (Logits)，形状为 (样本数, 类别数)</span></span><br><span class="line"><span class="comment"># 目标 y: 真实类别的索引 (Index)，形状为 (样本数)</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>]])  <span class="comment"># 模型预测各类的“信心”</span></span><br><span class="line">y = torch.tensor([<span class="number">1</span>])  <span class="comment"># 真实标签是索引 1 (比如“狗”)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CrossEntropyLoss 会自动帮我们做 Softmax + Log + NLLLoss</span></span><br><span class="line">loss_ce = nn.CrossEntropyLoss()</span><br><span class="line">res_ce = loss_ce(x, y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">对网络输出的样本分数使用softmax转换成概率</span></span><br><span class="line"><span class="string">对真实标签下的预测样本取负对数似然，即为损失</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--- 分类损失 (Classification Loss) ---"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"CrossEntropy Loss: <span class="subst">{res_ce.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2026/01/04/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" rel="prev" title="数据挖掘学习笔记：朴素贝叶斯">
      <i class="fa fa-chevron-left"></i> 数据挖掘学习笔记：朴素贝叶斯
    </a></div>
      <div class="post-nav-item">
    <a href="/2026/01/28/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/" rel="next" title="推荐系统学习笔记1：推荐系统基础">
      推荐系统学习笔记1：推荐系统基础 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><span class="nav-number">1.</span> <span class="nav-text">PyTorch学习笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="nav-number">1.1.</span> <span class="nav-text">数据读取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8dataset%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="nav-number">1.1.1.</span> <span class="nav-text">使用Dataset进行数据读取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8dataloader%E8%BF%9B%E8%A1%8C%E6%89%B9%E9%87%8F%E8%AF%BB%E5%8F%96"><span class="nav-number">1.1.2.</span> <span class="nav-text">使用DataLoader进行批量读取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8transforms%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="nav-number">1.1.3.</span> <span class="nav-text">使用Transforms进行数据转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#module%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">1.2.1.</span> <span class="nav-text">Module的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">1.2.2.</span> <span class="nav-text">池化的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%B1%82%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">1.2.3.</span> <span class="nav-text">线性层的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">1.2.4.</span> <span class="nav-text">非线性激活的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">1.3.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.3.1.</span> <span class="nav-text">反向传播手动实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.3.2.</span> <span class="nav-text">完整训练示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.3.</span> <span class="nav-text">损失函数</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ChuantaoLi"
      src="/uploads/%E7%8C%AB%E5%92%AA%E5%A4%B4%E5%83%8F.jpg">
  <p class="site-author-name" itemprop="name">ChuantaoLi</p>
  <div class="site-description" itemprop="description">感知幸福是一种能力</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ChuantaoLi</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">19k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:09</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
